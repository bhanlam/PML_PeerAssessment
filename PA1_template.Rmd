---
title: "Practical Machine Learning: Peer Assessment"
author: "Bhan Lam (lactobacillusnut)"
date: "26/07/2020"
output: 
    html_document:
        keep_md: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(caret)
library(dplyr)
library(randomForest)
```

## Synopsis

This document builds a classification model based on the Weight Lifting Exercise dataset as detailed in the background section below. A total of 5 classes were present in the dataset, i.e. factor with 5 levels. The dataset was imputed and further preprocessed before segmenting into training (70%) and validation (30%) sets. A random forest approach with 6-fold cross validation was employed to build a classifcation model. The final random forest model used 2 variables for splitting at each tree node (mtry = 27) with 99.06% accuracy. Using the model to predict the validation set yielded an accuracy of 99.52% with 0.48% out of sample error. The result was satisfactory and was used to predict the test set with 20 observations. 

## Background: Practical Machine Learning Course Project

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: [link](http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset).

## Loading and preprocessing the data

Firstly, download the training and test datasets. Read the datasets into a tibble. The packages <span style="color: red;">*dplyr*</span>, <span style="color: red;">*caret*</span>, and <span style="color: red;">*randomForest*</span> are loaded behind the scenes. 

There are 19622 observations and 160 variables in the training dataset and 20 observations and 160 variables in the test dataset.

There are also 5 classes in the *classe* variable, namely: exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E).

Out of the 19622 observations, there are only 406 complete cases. Hence, some cleaning is required to remove missing values.

```{r download}

# download file
if(!file.exists("pml-training.csv")) 
    download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv","pml-training.csv")
if(!file.exists("pml-testing.csv")) 
    download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv","pml-testing.csv")

# show dimensions
trngdataOrg <- tbl_df(read.csv("pml-training.csv")) 
dim(trngdataOrg)

testdataOrg <- tbl_df(read.csv("pml-testing.csv")) 
dim(testdataOrg)

names(trngdataOrg) #All 160 Variables
unique(trngdataOrg$classe) #Number of classes
ccases <- sum(complete.cases(trngdataOrg)) #number of complete cases
```

### Imputing data

The training data is subsetted to remove unnecessary variables or those with missing data. There are 67 variables that only contain non-NA values at 406 complete cases observations. These would be removed from the training dataset. It also appears that the first seven variables are not related to the data obatined from the sensors. Both the training and test dataset are subsetted such that both datasets contain the same set of 85 variables (except for the classe variable in the test dataset).

```{r subsetting}

#number of variables with NA values for all observations except the complete cases
sum(colSums(is.na(trngdataOrg))==19622-ccases) 

trngdata <- trngdataOrg[, -c(1:7)] #remove first 7 variables
testdata <- testdataOrg[, -c(1:7)] #remove first 7 variables

trngdata <- trngdata %>%
    select(which(colSums(is.na(.)) == 0))

subsettedVar <- names(trngdata)[1:85] #select all except classe variable

testdata <- testdata %>%
    select(subsettedVar) #subset test dataset to the same variable 

#check if the variables in both datasets are the same
sum(names(trngdata) %in% names(testdata))

```

### Near Zero Variance

In a further step, there are 33 variables with near zero variance, which would also be removed from the training and test sets. The resultant traning set would consist of 53 variables (52 in test set).

```{r nzv}

nzvVec <- nearZeroVar(trngdata, saveMetrics=TRUE)$nzv

#nvariables with near zero variances
names(trngdata)[nzvVec]

trngdata <- trngdata[, !nzvVec] #remove 33 nzv variables
testdata <- testdata[, !nzvVec[1:85]] #remove 33 nzv variables

```

### Data Segmentation

Since there is a final test set provided, we will first split the training dataset into pure training (70%) and a validation set (30%).

```{r seg}

set.seed(9875)
puretrng <- createDataPartition(trngdata$classe, p=0.70, list=FALSE)
pureTrngData <- trngdata[puretrng, ]
valData <- trngdata[-puretrng, ]
```

## Random forest approach

We start off with a random forest approach for this classification problem. Since the dataset is rather larger, we adopt a 6-fold cross validation approach with an assumption that the dataset is drawn from the same distribution. Preprocessing to centre and scale the variables was also included. The final model used 27 variables for splitting at each tree node (mtry = 27) with 99.06% accuracy.

```{r randomForest, echo=TRUE}
set.seed(99)
cv <- trainControl(method="cv", 6) #cross validation
rfModel <- train(classe ~ ., data=pureTrngData, method="rf", preProcess=c("center", "scale"), trControl=cv, ntree=250)
rfModel
rfModel$finalModel
plot(rfModel)
```

### Validation

The validation set is used to validate the trained model "rfModel". It appears that the accuracy is 99.52% and the out of sample error is 0.48%.

```{r validationRF}
predictValRF <- predict(rfModel, newdata=valData)
confMatRF <- confusionMatrix(predictValRF, as.factor(valData$classe))
confMatRF

#out of sample error
1-as.numeric(confMatRF$overall[1])

```

### 20 Sample Test Set

Hence the 20 sample test set is predicted as follows.

```{r predictTestRF}
predictTestRF <- predict(rfModel, testdata)
predictTestRF

```

